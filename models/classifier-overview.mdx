---
title: "Classifiers overview"
sidebarTitle: "Overview"
---

Classifiers are used to label a text with a specific category. For example, you can use a classifier to label a text as positive or negative.

### Response signature

Each classifier model returns an object with the detected label and the respective scores. This object is added to the API response under the model's key.

<ResponseField name="label" type="string" required>
  The most probable label. This will always be the label with the highest score.
  Returns null if the analyzer fails.
</ResponseField>
<ResponseField name="score" type="number" required>
  The score of the label. From 0-1 score with 1 meaning a high probability of
  being correct.
</ResponseField>
<ResponseField name="label_scores" type="object" required>
  An object containing all the label scores.
</ResponseField>

## Pre-built text classifiers

<CardGroup cols={3}>
  <Card title="Toxicity" href="/models/toxicity" icon="fire">
    Detects toxic content.
  </Card>

<Card title="NSFW" href="/models/nsfw" icon="flag">
    Detects NSFW content.

</Card>
<Card title="Propriety" href="/models/propriety" icon="hand">
    Detects inappropriate content.
  </Card>

<Card title="Sentiment" href="/models/sentiment" icon="heart">
  Positive, negative, or neutral.
</Card>

<Card title="Spam" href="/models/spam" icon="star">
  Detects spam.
</Card>

<Card title="Language" href="/models/language" icon="language">
  Detects the language.
</Card>

<Card title="Sexual" href="/models/sexual" icon="venus-mars">
  Detects sexual content.
</Card>

<Card
  title="Discrimination"
  href="/models/discrimination"
  icon="hand-point-right"
>
  Detects discriminating content.
</Card>

<Card title="Self harm" href="/models/self-harm" icon="face-frown">
  Detects content related to self harm.
</Card>

<Card title="Violence" href="/models/violence" icon="hand-fist">
  Detects content related to violence.
</Card>

</CardGroup>
