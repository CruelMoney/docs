---
title: "Quickstart"
description: "Start moderating your content in under 5 minutes"
---

## 1. Create a project

1. Visit the [dashboard](https://moderationapi.com/app) and create a new project.
2. You can either create a project using one of the templates or opt for a blank project.
3. Add one or more models to handle different content types or classification tasks. For example, use the `toxicity` model to detect harmful language and the `profanity` model to catch offensive words.

<Frame>
  <img
    className="block "
    src="/images/project-page.png"
    alt="Content moderation project"
  />
</Frame>

> Tip: If you’re not sure which models to choose, start with the most commonly used one (toxicity) and expand later.

---

## 2. Submit content for analysis

Once your project is set up, you can begin sending text, images, or other media to your models for moderation. There are several ways to do this:

### Option A: Using the API Directly

To send content to the API, you’ll need:

- Your project’s API key, which you can find under the **Settings** tab on your project page.
- The `/api/v1/moderate/text` endpoint to moderate text content (use the corresponding endpoints for other content types).

```javascript Fetch example
const API_KEY = "YOUR_SECRET_API_KEY_HERE";

const response = await fetch("https://moderationapi.com/api/v1/moderate/text", {
  method: "POST",
  headers: {
    "Content-Type": "application/json",
    Authorization: `Bearer ${API_KEY}`,
  },
  body: JSON.stringify({
    value: "Hello world!",
    authorId: "123", // optional
    contextId: "456", // optional
    metadata: {
      source: "forum-post",
    },
  }),
});

if (!response.ok) {
  // Handle the error (e.g., invalid API key, network error, etc.)
  throw new Error("Failed to moderate content.");
}

const { flagged, categories } = await response.json();

if (flagged) {
  // Return an error to the user, or block the content
  console.error("Content was flagged:", categories);
} else {
  // Approve the content, send it to your DB, etc.
  console.log("Content is safe to display.");
}
```

### Option B: Using the Node.js SDK

Or use one of our [server SDKs](/api-reference/introduction#sdks). Below is a minimal Node.js example that demonstrates text and image moderation via the SDK:

```typescript Node.js example
import ModerationApi from "@moderation-api/sdk";

// Configure your client using an environment variable for the API key
const moderationApi = new ModerationApi({
  key: process.env.API_KEY,
});

try {
  // Text moderation
  const textAnalysis = await moderationApi.moderate.text({
    value: "Hello world!",
    authorId: "123",
    contextId: "456",
    metadata: {
      customField: "value",
    },
  });

  if (textAnalysis.flagged) {
    console.warn("Text content flagged:", textAnalysis.categories);
    // Block the content, show an error, etc...
  } else {
    console.log("Text content is safe.");
    // Save to database or proceed...
  }

  // Image moderation
  const imageAnalysis = await moderationApi.moderate.image({
    url: "https://example.com/image.jpg",
    authorId: "123",
    contextId: "456",
    metadata: {
      customField: "value",
    },
  });

  if (imageAnalysis.flagged) {
    console.warn("Image content flagged:", imageAnalysis.categories);
    // Block or require review
  } else {
    console.log("Image is safe.");
    // Save to database or proceed...
  }
} catch (error) {
  console.error("Error invoking moderation API:", error);
}
```

> Note: Always store your API key securely and rotate it regularly to maintain security.

### Option D: Using an Integration

We also provide handy integrations to streamline content moderation in popular platforms:

- Zapier: Submit content from Google Sheets, Airtable, etc.
- WordPress Plugin: Automatically moderate comments, reviews, and form entries.

See our [integrations page](https://moderationapi.com/integrations) for all available options.

---

> **Dry-run Mode:** If you want to analyze production data without enforcing moderation outcomes, enable “dry-run” in your project settings. This ensures the API always returns `flagged: false` when you submit content, yet still logs the analysis in your dashboard. Perfect for pre-production experiments or diagnosing how the model flags real-world data.

---

## 3. Review flagged content (optional)

If content is flagged by any of your models, it will appear in the [Review Queue](/review-queues). This allows you to blend automated and human moderation:

<Frame>
  <img
    className="block "
    src="/images/content-queue.png"
    alt="Review queue for reviewing and improving automated moderation"
  />
</Frame>

In the review queue, you can:

- Sanity-check items before they’re published.
- Override automatic rejections if they’re false positives.
- Identify repeat offenders and ban them.
- Improve your models by correcting misclassifications.
- Invite team members or moderators to help review flagged items.

---

## All Done!

Congratulations! You’ve run your first moderation checks. Here are a few next steps:

- Continue tweaking your project settings and models as your platform grows.
- Explore advanced features like real-time notifications or custom classification.
- If you have questions, reach out to our [support team](mailto:support@moderationapi.com).

We love hearing from you—please share how you’re using the service and let us know if you have suggestions or need help. Happy moderating!
