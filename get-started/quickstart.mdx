---
title: "Quickstart"
description: "Start moderating your content in under 5 minutes"
---

## 1. Create a project

Visit the [dashboard](https://moderationapi.com/app) and create a new project. You can either create a project using one of the templates or create a blank project.

You can add multiple models to a project. Each model will detect a different type of content. For example, you can create a project with a `toxicity` model and a `profanity` model to prevent innapropriate content from being posted on your platform.

<Frame>
  <img
    className="block "
    src="/images/project-page.png"
    alt="Content moderation project"
  />
</Frame>

## 2. Submit content for analysis

After configuring up your project you can submit content to the project for analysis.

<Tabs>

<Tab icon="rocket" title="Using the dashboard">
  The easiest way to submit content for analysis is to use the dashboard. You
  can submit content for analysis by clicking on the `Send API request` button
  on the project page. <br></br>
  <br></br>This is mainly useful for testing and debugging purposes while configuring
  your project.
</Tab>

<Tab icon="code" title="Using the API">
  To submit content using the API you'll need your project's API key. You can
  find your project's API key on the project page under the `Integrate` section.

<Frame>
  <img className="block " src="/images/api-key.png" alt="Api-key" />
</Frame>

Use the API key to authenticate your requests to the API. You can submit content for analysis by sending a `POST` request to the `/api/v1/moderate/text` endpoint.

```javascript Text moderation example
const data = await fetch("https://moderationapi.com/api/v1/moderate/text", {
  method: "POST",
  headers: {
    Authorization: `Bearer ${API_KEY}`,
  },
  body: JSON.stringify({
    value: "Hello world!",
  }),
});

const { flagged } = await data.json();

if (flagged) {
  // Return error to user etc.
} else {
  // Add to database etc.
}
```

Utilize the `flagged` field to determine how to handle the content. For example if the `flagged` field is `true` then the content should be blocked. If the `flagged` field is `false` then the content is safe to be posted.

</Tab>

<Tab icon="rocket" title="Using an integration">
  You can also submit content using one of our integrations. <br></br>
  
  For example you can use Zapier to submit content from your favorite apps like Google Sheets, Airtable, and more. <br></br>
  
  Or use our WordPress plugin to automatically moderate comments and reviews on your website. <br></br>
  
  
   See which integrations are available on our [integrations page](https://moderationapi.com/integrations).

</Tab>

</Tabs>

## 3. Review flagged content (optional)

Content that is flagged by the model will be sent to the **Content Queue**. This is useful if you want to combine automated content moderation with human moderation, or if you simply want to review what content is being flagged.<br></br>

<Frame>
  <img
    className="block "
    src="/images/content-queue.png"
    alt="Content queue for reviewing and improving automated moderation"
  />
</Frame>

The content queue can be used for a variety of use cases:

- **Human moderation** - Review content that is flagged by the model and decide whether to block or allow the content.
- **Improve custom models** - Review content that is flagged by the model and provide feedback to the model. This will help the model learn and improve over time.
- **Manual actions** - Ban, block, or warn users based on the content they post.
- **Multiple queues** - Create multiple queues for different use cases. For example, you can create a queue for each language that you support or different types of content.
- **Invite moderators** - Invite your team or hire moderators to help you review content in the queue.

## All done!

Congrats! Youâ€™ve now automated your content moderation! Need support or want to give some feedback? You can drop us an email at [support@moderationapi.com](mailto:support@moderationapi.com).
